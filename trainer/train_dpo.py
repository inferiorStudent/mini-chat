import os
import json
import torch
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer

from model.model_slm import SLMForCausalLM


############################################
# Initialize the configuration
############################################

if os.path.exists("./config/dpo_config.json"):
    with open('./config/dpo_config.json', 'r') as f:
        CONFIG = json.load(f)
else:
    CONFIG = None


class DPODataset(Dataset):
    def __init__(self, data_path: str, tokenizer: AutoTokenizer, max_len: int=512):
        self.data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip(): continue
                try:
                    item = json.loads(line)
                    self.data.append(item)
                except json.JSONDecodeError:
                    continue
        self.tokenizer = tokenizer
        self.max_len = max_len
    
    def __len__(self):
        return len(self.data)
    
    def _process(self, prompt: str, answer):
        prompt_text = f"{self.tokenizer.bos_token}user\n{prompt}{self.tokenizer.eos_token}\n{self.tokenizer.bos_token}assistant\n"
        full_text = prompt_text + f"{answer}{self.tokenizer.eos_token}"

        # åˆ†åˆ«ç¼–ç 
        prompt_ids = self.tokenizer.encode(prompt_text, add_special_tokens=False)
        full_ids = self.tokenizer.encode(full_text, add_special_tokens=False)

        if len(full_ids) > self.max_len:
            full_ids = full_ids[:self.max_len]
        
        pad_len = self.max_len - len(full_ids)
        input_ids = full_ids + [self.tokenizer.pad_token_id] * pad_len
        attention_mask = [1] * len(full_ids) + [0] * pad_len

        # promptå’Œpaddingéƒ¨åˆ†ä¸º0 answerä¸º1
        prompt_len = len(prompt_ids)
        labels_mask = [0] * prompt_len + [1] * (len(full_ids) - prompt_len) + [0] * pad_len
        labels_mask = labels_mask[:self.max_len]

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels_mask": torch.tensor(labels_mask, dtype=torch.float)
        }
    
    def __getitem__(self, index):
        item = self.data[index]
        chosen = self._process(item['prompt'], item['chosen'])
        rejected = self._process(item['prompt'], item['rejected'])
        return {
            "chosen_ids": chosen["input_ids"],
            "chosen_mask": chosen["attention_mask"],
            "chosen_labels_mask": chosen["labels_mask"],

            "rejected_ids": rejected["input_ids"],
            "rejected_mask": rejected["attention_mask"],
            "rejected_labels_mask": rejected["labels_mask"]
        }


def dpo_loss(policy_chosen_logps, policy_rejected_logps, ref_chosen_logps, ref_rejected_logps, beta=0.1):
    '''
        DPO loss = -log(sigmoid(beta * (log(pi_chosen / ref_chosen) - log(pi_rejected / ref_rejected))))
    '''
    # ç­–ç•¥æ¨¡å‹ åå¥½å·®
    policy_log_ratios = policy_chosen_logps - policy_rejected_logps
    # å‚è€ƒæ¨¡å‹ åå¥½å·®
    ref_log_ratios = ref_chosen_logps - ref_rejected_logps

    # policyåå¥½å·® > refåå¥½å·®
    losses = -F.logsigmoid(beta * (policy_log_ratios - ref_log_ratios))
    return losses.mean()

def get_batch_logps(model, input_ids, attention_mask, labels_mask):
    '''
        batchçš„log p
        labels_mask: (batch_size, seq_len), answeréƒ¨åˆ†ä¸º1 åªè®¡ç®—answeréƒ¨åˆ†çš„losså€¼
    '''
    outputs = model(input_ids, attention_mask=attention_mask)
    logits = outputs.logits # (batch_size, seq_len, vocab_size)

    labels = input_ids[:, 1:].clone()
    logits = logits[:, :-1, :]

    loss_mask = labels_mask[:, 1:] # å’Œlabelsä¸€æ ·çš„ä½ç§»
    
    # è·å–æ¯ä¸ªtokençš„log softmax
    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)
    # åªè®¡ç®—answeréƒ¨åˆ†
    return (per_token_logps * loss_mask).sum(-1)



def train_dpo():
    if CONFIG is None:
        print("âŒ è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        return
    
    if not os.path.exists(CONFIG["model_path"]):
        print(f"âŒ æ¨¡å‹ {CONFIG['model_path']} ä¸å­˜åœ¨, è¯·æ£€æŸ¥ä½ çš„è·¯å¾„")
        return
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(CONFIG["model_path"])
    except:
        tokenizer = AutoTokenizer.from_pretrained("./out")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # æ¨¡å‹åŠ è½½ä¸¤æ¬¡ï¼Œåˆ†åˆ«ä½œä¸ºpolicyå’Œref
    policy_model = SLMForCausalLM.from_pretrained(CONFIG["model_path"]).to(device)
    ref_model = SLMForCausalLM.from_pretrained(CONFIG["model_path"]).to(device)
    ref_model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    if not os.path.exists(CONFIG["data_path"]):
        print(f"âŒ æ–‡ä»¶ {CONFIG['data_path']} ä¸å­˜åœ¨")
        return
    dataset = DPODataset(CONFIG["data_path"], tokenizer)
    dataloader = DataLoader(dataset, batch_size=CONFIG["batch_size"], shuffle=True, drop_last=False)

    optimizer = AdamW(policy_model.parameters(), lr=CONFIG["learning_rate"])
    policy_model.train()

    print("ğŸ”¥ å¼€å§‹å¼ºåŒ–è®­ç»ƒ")
    for epoch in range(CONFIG["epochs"]):
        progress_bar = tqdm(dataloader, total=len(dataloader), desc=f"epoch {epoch}")
        for batch in progress_bar:
            c_ids, c_mask = batch["chosen_ids"].to(device), batch["chosen_mask"].to(device)
            r_ids, r_mask = batch["rejected_ids"].to(device), batch["rejected_mask"].to(device)
            c_labels_mask = batch["chosen_labels_mask"].to(device)
            r_labels_mask = batch["rejected_labels_mask"].to(device)

            policy_chosen_logps = get_batch_logps(policy_model, c_ids, c_mask, c_labels_mask)
            policy_rejected_logps = get_batch_logps(policy_model, r_ids, r_mask, r_labels_mask)

            with torch.no_grad():
                ref_chosen_logps = get_batch_logps(ref_model, c_ids, c_mask, c_labels_mask)
                ref_rejected_logps = get_batch_logps(ref_model, r_ids, r_mask, r_labels_mask)
            
            loss = dpo_loss(policy_chosen_logps, policy_rejected_logps, ref_chosen_logps, ref_rejected_logps, beta=CONFIG["beta"])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            progress_bar.set_postfix(loss=f"{loss.item():.4f}")
    
    policy_model.save_pretrained(os.path.join(CONFIG["output_dir"], "dpo_model"))
    # tokenizer.save_pretrained(CONFIG["output_dir"])
    print("ğŸš© å®ŒæˆDPOè®­ç»ƒ")