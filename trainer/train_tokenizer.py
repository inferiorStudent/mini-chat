import os
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors
from transformers import PreTrainedTokenizerFast, AutoTokenizer

###################################
# è¯¥è„šæœ¬å¯ä»¥å•ç‹¬è¿è¡Œ
###################################

VOCAB_SIZE = 32000

# æ›¿æ¢æˆä½ è‡ªå·±è¦è®­ç»ƒçš„è‹±æ–‡è¯­æ–™
CORPUS_FILE_LIST = [
    # 'data/raw/wikipedia/enwiki-20251220-1.txt',
    # 'data/raw/wikipedia/enwiki-20251220-2.txt',
    # 'data/raw/wikipedia/enwiki-20251220-3.txt'
]

CHINESE_VOCAB_PATH = 'dataset/3500.txt' # 3500å¸¸ç”¨æ±‰å­—

OUTPUT_DIR = './out' # è·¯å¾„ç›¸å¯¹äºå·¥ä½œè·¯å¾„è€Œè¨€
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è¿­ä»£è¯»å…¥æ–‡ä»¶
def data_iterator(batch_size=1000):
    for file_path in CORPUS_FILE_LIST:
        with open(file_path, 'r', encoding='utf-8') as file:
            batch = []
            for line in file:
                if len(line) == 0:
                    continue
                batch.append(line)
                if len(batch) >= batch_size:
                    yield batch
                    batch = []
            if batch:
                yield batch

# ç›´æ¥å°†ä¸€ä¸ªæ±‰å­—ä½œä¸ºä¸€ä¸ªtoken
def get_chinese_vocab(file_path: str) -> list[str]:
    chinese_vocab = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            chinese_vocab += list(set(list(line)))
    return chinese_vocab

def train_tokenizer():
    print("ğŸš€ åˆå§‹åŒ–åˆ†è¯å™¨")
    chinese_vocab = get_chinese_vocab(CHINESE_VOCAB_PATH)

    tokenizer = Tokenizer(models.BPE(unk_token='<unk>'))

    # ByteLevel: GPT-2/Llamaå°†æ–‡æœ¬è½¬åŒ–ä¸ºå­—èŠ‚æµ
    # å­—èŠ‚æµçš„å¥½å¤„å°±æ˜¯æ²¡è§åˆ°è¿‡çš„tokenä¸æ€»æ˜¯è¢«è¯†åˆ«ä¸º<unk>
    # tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.pre_tokenizer = pre_tokenizers.Sequence([
        pre_tokenizers.ByteLevel(add_prefix_space=False),
    ])
    tokenizer.decoder = decoders.ByteLevel()

    special_tokens = ['<unk>', '<|im_start|>', '<|im_end|>', '<pad>']
    trainer = trainers.BpeTrainer(
        vocab_size = VOCAB_SIZE - len(chinese_vocab) - 100,
        special_tokens=special_tokens,
        min_frequency=100,
        limit_alphabet=1500,
        show_progress=True,
    )

    for file_path in CORPUS_FILE_LIST:
        if not os.path.exists(file_path):
            print(f"{file_path} ä¸å­˜åœ¨, è¯·æ£€æŸ¥æ‹¼å†™")
            return None

    # tokenizer.train(files=CORPUS_FILE_LIST, trainer=trainer)
    tokenizer.train_from_iterator(data_iterator(), trainer=trainer)
    print("âœ… è®­ç»ƒå®Œæˆ")
    tokenizer.add_tokens(chinese_vocab)

    # é¦–å°¾è‡ªåŠ¨æ·»åŠ ç‰¹æ®Štoken
    # tokenizer.post_process = processors.TemplateProcessing(
    #     single="<s>$A</s>",
    #     pair="<s>$A</s>$B</s>",
    #     special_tokens=[
    #         ("<s>", tokenizer.token_to_id("<s>")),
    #         ("</s>", tokenizer.token_to_id("</s>"))
    #     ]
    # )

    save_path = os.path.join(OUTPUT_DIR, "tokenizer.json")
    tokenizer.save(save_path)
    print(f"âœ… tokenizer.json å·²ç»ä¿å­˜è‡³ {OUTPUT_DIR}")

    print("ğŸ”„è½¬æ¢ä¸ºHugging Faceæ ‡å‡†")
    wrapped_tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        tokenizer_file=save_path,
        bos_token="<|im_start|>",
        eos_token="<|im_end|>",
        unk_token="<unk>",
        pad_token="<pad>"
    )
    # config.json, special_tokens_map.json
    wrapped_tokenizer.save_pretrained(OUTPUT_DIR)
    print("âœ… å·²ç»å®Œæˆ")

    return wrapped_tokenizer

def test_tokenizer() -> None:
    print("\nğŸ§ª æµ‹è¯•åŠ è½½ä¸ç¼–ç ")
    try:
        tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)
    except:
        print("âŒ æ²¡æœ‰å‘ç°è¯è¡¨, è¯·æ£€æŸ¥æ˜¯å¦å­˜åœ¨")
        return
    print(f"è¯è¡¨å¤§å°ä¸º: {len(tokenizer)}")
    # tokenizer.size å’Œ len(tokenizer) ä¸åŒ, å‰è€…æ²¡æœ‰åŒ…å«ç›´æ¥åŠ è¿›å»çš„token
    text = "â€œå¾ˆé«˜å…´è§åˆ°ä½ â€çš„è‹±æ–‡è¡¨è¾¾æ˜¯â€œNice to meet youâ€ã€‚"

    encoded = tokenizer.encode(text)
    print(f"ç¼–ç åçš„ç»“æœ: {encoded}")
    decoded = tokenizer.decode(encoded)
    print(f"è§£ç åçš„ç»“æœ: {decoded}")

if __name__ == "__main__":
    # res = train_tokenizer()
    # if res is None:
    #     print(f"âŒ åˆ†è¯å¤±è´¥, è¯·é‡æ–°å°è¯•")
    
    test_tokenizer()