import torch
from transformers import AutoTokenizer

from model.model_slm import SLMForCausalLM

def chat():
    MODEL_PATH = "./out/dpo_model" 
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ åŠ è½½æ¨¡å‹ {MODEL_PATH} åˆ° {device}...")

    # 1. åŠ è½½åˆ†è¯å™¨
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    except:
        tokenizer = AutoTokenizer.from_pretrained("./out")

    model = SLMForCausalLM.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map=device
    )
    
    # è®¾ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    print("\nğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºå†å² (æ¯æ¬¡éƒ½æ˜¯æ–°å¯¹è¯)")
    print("-" * 50)

    # 3. äº¤äº’å¾ªç¯
    while True:
        prompt = input("\nğŸ‘¤ User: ").strip()
        if prompt.lower() == "exit":
            break
        if not prompt:
            continue
        
        # æ˜¾å¼æ„é€ å¯¹è¯æ ¼å¼, æœªæ¥åœ¨åˆ†è¯å™¨æ–‡ä»¶ä¸­å®ç°
        input_text = f"{tokenizer.bos_token}user\n{prompt}\n{tokenizer.eos_token}\n{tokenizer.bos_token}assistant\n"

        inputs = tokenizer(input_text, return_tensors="pt").to(device)
        # if "token_type_ids" in inputs:
        #     del inputs["token_type_ids"]
        with torch.no_grad():
            output_ids = model.generate(
                **inputs,
                max_new_tokens=100,
                
                do_sample=True,
                temperature=0.7,
                top_k=50,
                top_p=0.9,
                
                repetition_penalty=1.1, # é˜²æ­¢å¤è¯»æœº
                
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )

        new_tokens = output_ids[0][inputs.input_ids.shape[1]:]
        response = tokenizer.decode(new_tokens, skip_special_tokens=True)

        print(f"ğŸ¤– AI: {response}")
        break

def chat_():
    from transformers import TextIteratorStreamer
    from threading import Thread
    MODEL_PATH = "out/dpo_model" 
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ åŠ è½½æ¨¡å‹ {MODEL_PATH} åˆ° {device}...")

    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    except:
        tokenizer = AutoTokenizer.from_pretrained("./out")

    model = SLMForCausalLM.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map=device
    )
    
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

    print("\nğŸ’¡ è¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºå†å² (æ¯æ¬¡éƒ½æ˜¯æ–°å¯¹è¯)")
    print("-" * 50)

    while True:
        prompt = input("\nğŸ‘¤ User: ").strip()
        if prompt.lower() == "exit":
            break
        if not prompt:
            continue

        prompt_str = f"{tokenizer.bos_token}user\n{prompt}\n{tokenizer.eos_token}\n{tokenizer.bos_token}assistant\n"
        
        input_text = prompt_str
        inputs = tokenizer(input_text, return_tensors="pt").to(device)
        with torch.no_grad():
            streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=False)
            generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=512, do_sample=True, temperature=0.8, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id,)

            # å¿…é¡»åœ¨çº¿ç¨‹ä¸­è¿è¡Œ generateï¼Œå¦åˆ™ä¼šé˜»å¡
            thread = Thread(target=model.generate, kwargs=generation_kwargs)
            thread.start()

            print("ğŸ¤– AI: ", end="", flush=True)
            for new_text in streamer:
                print(new_text, end="", flush=True)
            print()


def chat_origin():
    from model import origin_model_slm
    MODEL_PATH = "./out/dpo_model"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # device = "cpu"
    print(f"ğŸš€ åŠ è½½æ¨¡å‹ {MODEL_PATH} åˆ° {device}...")

    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    except:
        tokenizer = AutoTokenizer.from_pretrained("./out")
    
    model = origin_model_slm.SLMForCausalLM.from_pretrained(
        MODEL_PATH, 
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map=device
    )

    model.eval()
    prompt = "ä½ å¥½ï¼Œ"
    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)
    outputs = model.generate(input_ids, max_new_tokens=50, eos_token_id=tokenizer.eos_token_id)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))