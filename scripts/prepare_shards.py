import os
import glob
import random
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer

####################################
# è¯¥è„šæœ¬å¯ä»¥å•ç‹¬è¿è¡Œ
####################################

CONFIG = {
    "source_files": [
        # "data/raw/wikipedia/enwiki-20251220-1.txt",
        # "data/raw/wikipedia/enwiki-20251220-2.txt",
        # "data/raw/wikipedia/enwiki-20251220-3.txt",
        # "data/raw/wikipedia/zhwiki-20251220-6.txt",
        # "data/raw/peoples-daily/peoples-daily-corpus.txt",
        "./dataset/temp.txt"
    ],
    "tokenizer_path": "./out",
    "output_dir": "dataset/processed",
    "shard_tokens": 100_000_000, # 1ä¸ªtokenå 2ä¸ªByte -> 1ä¸ªæ•°æ®æ–‡ä»¶200M
}

def process_data_into_bin():
    '''
    æ–‡æœ¬é¢„å¤„ç†: 
        å°†æ–‡æœ¬è½¬æ¢ä¸ºtokenå¹¶ç¼–ç , vocabå¤§å°ä¸º31900 å› æ­¤å¯ä»¥ç”¨uint16æ¥å­˜å‚¨æ¯ä¸ªtoken
        ç”±äºæ•°æ®æ–‡ä»¶å¤ªå¤§, å› æ­¤å°†è¯­æ–™æ··åˆå¹¶åˆ‡åˆ†ä¸ºå¤šä¸ªäºŒè¿›åˆ¶æ–‡ä»¶
    '''
    os.makedirs(CONFIG["output_dir"], exist_ok=True)
    tokenizer = AutoTokenizer.from_pretrained(CONFIG["tokenizer_path"])

    all_lines = []
    print(" æ··åˆæºæ–‡ä»¶")
    # CONFIG["sorce_files"] = glob.glob(os.path.join("dir", "*.txt"))
    for file_path in CONFIG["source_files"]:
        if not os.path.exists(file_path):
            print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
            continue
        
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
            lines = [line.strip() for line in lines if len(line.strip()) > 0]
            all_lines.extend(lines)
            print(f"   - {file_path}: {len(lines)} lines")
    
    print(f"ğŸ² æ··åˆæ‰“ä¹± {len(all_lines)} è¡Œæ–‡æœ¬")
    random.shuffle(all_lines) # æ•°æ®é‡å¤ªå¤§çš„è¯ä¸å»ºè®®è¿™ä¹ˆåš å› ä¸ºå†…å­˜ä¸å¤Ÿ

    print("âš™ï¸ åˆ†è¯å¹¶æ„å»ºåˆ†ç‰‡")
    current_token_ids = []
    shard_index = 0
    eos_id = tokenizer.eos_token_id

    batch_size = 5000
    last_batch_index = len(all_lines) // batch_size
    for i in tqdm(range(0, len(all_lines), batch_size)):
        batch = all_lines[i : i + batch_size]
        encoded = tokenizer(batch, add_special_tokens=False)["input_ids"]
        for ids in encoded:
            current_token_ids.extend(ids)
            current_token_ids.append(eos_id)
        
        target_token_count = CONFIG["shard_tokens"]
        while len(current_token_ids) >= target_token_count:
            save_ids = current_token_ids
            current_token_ids = []
            save_path = os.path.join(CONFIG["output_dir"], f"shard_{shard_index:03d}.bin")
            print(f"ğŸ’¾ ä¿å­˜åˆ†ç‰‡ {save_path}")
            arr = np.array(save_ids, dtype=np.uint16)
            with open(save_path, "wb") as f:
                f.write(arr.tobytes())
            shard_index += 1
    
    # å¤„ç†æœ€åä¸€å¨æ•°æ®
    last_batch = all_lines[last_batch_index * batch_size:]
    last_batch_encoded = tokenizer(last_batch, add_special_tokens=False)["input_ids"]
    for ids in last_batch_encoded:
        current_token_ids.extend(ids)
        current_token_ids.append(eos_id)
    if len(current_token_ids) == 0:
        return
    
    save_path = os.path.join(CONFIG["output_dir"], f"shard_{shard_index:03d}.bin")
    print(f"ğŸ’¾ ä¿å­˜æœ€åä¸€ä¸ªåˆ†ç‰‡ {save_path}")
    arr = np.array(current_token_ids, dtype=np.uint16)
    with open(save_path, "wb") as f:
        f.write(arr.tobytes())
    
    print("âœ… æ•°æ®å¤„ç†å®Œæˆ!")


if __name__ == "__main__":
    process_data_into_bin()